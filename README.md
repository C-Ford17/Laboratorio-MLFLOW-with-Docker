# Laboratorio: App de TraducciÃ³n con Docker y MLflow

AplicaciÃ³n de traducciÃ³n de texto usando **Gemini API** con tracking en **MLflow**, empaquetada en **contenedores Docker** y publicada en **Docker Hub**.

## ðŸ“‹ DescripciÃ³n

Esta aplicaciÃ³n permite traducir texto a mÃºltiples idiomas utilizando el modelo Gemini 2.5-flash. Cada traducciÃ³n se registra automÃ¡ticamente en MLflow para tracking, incluyendo parÃ¡metros, mÃ©tricas y artifacts.

**CaracterÃ­sticas:**
- âœ… Interfaz web con Gradio
- âœ… Traducciones con Gemini API
- âœ… Tracking completo en MLflow
- âœ… DockerizaciÃ³n sin docker-compose
- âœ… PublicaciÃ³n en Docker Hub
- âœ… EjecuciÃ³n remota

## ðŸ—ï¸ Arquitectura

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docker Network â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ traductor-app â”‚ â”‚ mlflow-server â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ Puerto: 7860 â”‚â”€â”€â”€â”€â†’â”‚ Puerto: 5000 â”‚ â”‚
â”‚ â”‚ Gradio UI â”‚ â”‚ MLflow Tracking UI â”‚ â”‚
â”‚ â”‚ (Python 3.10) â”‚ â”‚ (Python 3.10) â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â†“ â”‚
â”‚ Volumen: mlflow_data â”‚
â”‚ (/mlflow/mlruns) â”‚
â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


## ðŸš€ Inicio RÃ¡pido

### Requisitos
- Docker 20.10+
- Docker Hub account (para publicaciÃ³n)
- Google Gemini API key ([obtener aquÃ­](https://ai.google.dev/))

### Paso 1: Clonar el repositorio

git clone https://github.com/C-Ford17/Laboratorio-MLFLOW-with-Docker.git
cd Laboratorio-MLFLOW-with-Docker


### Paso 2: Crear archivo `.env`


Cuenta

Actualizar
Instalar
vale ya tengo las capturas de pantalla dame el readme con los comandos exactos y dame mis comandos para subirlo a mi github C-Ford17/Laboratorio-MLFLOW-with-Docker
hola tengo una tarea por hacer, no se si en un notebook o app de python
Taller: App de TraducciÃ³n con Docker 
DescripciÃ³n general
ConstruirÃ¡s una aplicaciÃ³n de traducciÃ³n de texto que usa un modelo generativo (Gen-AI) a travÃ©s del SDK compatible (p. ej. OpenAI â†” Gemini), registra cada interacciÃ³n con MLflow Tracking, y se empaqueta y despliega solo con imÃ¡genes y contenedores Docker (sin docker-compose). AdemÃ¡s deberÃ¡s publicar la imagen de la app en Docker Hub y demostrar ejecuciÃ³n remota.
Requisitos obligatorios
La app debe exponer una interfaz web con Gradio donde el usuario:
ingrese un texto fuente,
seleccione el idioma objetivo,
reciba la traducciÃ³n generada por el modelo.
Cada interacciÃ³n (prompt) debe registrarse en MLflow incluyendo, como mÃ­nimo:
texto original,
idioma objetivo,
texto traducido generado,
timestamp,
mÃ©tricas opcionales (latencia, longitud de respuesta).
Todo debe correr en contenedores Docker individuales (un contenedor para la app y otro para el servidor MLflow), sin usar docker-compose.
Debes construir la imagen local, ejecutar contenedores, taggear y subir la imagen de la app a Docker Hub, y demostrar que la imagen se puede ejecutar en otra mÃ¡quina (pasando la API key por variable de entorno).
Entregables (obligatorios)
Capturas de pantalla: UI de Gradio con una traducciÃ³n y la entrada correspondiente en la UI de MLflow (con al menos 1 run).
Nombre del repo en Docker Hub con la imagen publicada (p. ej. tu_usuario/traductor-genai:1.0.0).
Documento breve (mÃ¡x. 1 pÃ¡gina) que explique: arquitectura (contenedores y puertos), cÃ³mo pasar la API key, comandos principales usados (build, run, tag, push, pull), y observaciones sobre latencia y calidad de traducciÃ³n.
Estructura del trabajo (partes)
Parte A â€” App de traducciÃ³n (desarrollo local)
DiseÃ±ar la interfaz Gradio (campo texto, selector idioma, botÃ³n traducir, Ã¡rea de resultado).
Integrar llamadas al SDK compatible para obtener la traducciÃ³n generada.
Probar localmente sin Docker hasta que funcione.
Parte B â€” MLflow Tracking (obligatorio)
Levantar un servidor MLflow en contenedor separado.
En la app, al generar una traducciÃ³n, crear un run en MLflow que registre:
parÃ¡metros: idioma_objetivo, modelo, lenguaje_origen (si aplica), prompt_hash (opcional),
mÃ©tricas: latency_ms, len_response, (opcionales),
artifacts: (opcional) guardar un txt con la pareja original/traducciÃ³n.
Verificar que los runs aparecen en la UI de MLflow.
Parte C â€” DockerizaciÃ³n sin docker-compose
Crear Dockerfile para la app (base Python, instalar deps, exponer puerto 7860).
Crear Dockerfile o usar imagen oficial para MLflow server (montar volumen para persistencia).
Ejecutar contenedores y asegurar que la app apunte al servidor MLflow usando la URL del contenedor MLflow (por nombre de host cuando ambos corren en la misma red Docker) â€” todo manejado con comandos docker network/docker run (no compose).
Parte D â€” PublicaciÃ³n en Docker Hub y ejecuciÃ³n remota
Hacer login en Docker Hub, taggear la imagen de la app y docker push.
En otra mÃ¡quina, docker pull y ejecutar la app pasando la API key como variable de entorno; verificar que las traducciones se registren en el MLflow remoto (o en el servidor MLflow que levantes remotamente).
Criterios de evaluaciÃ³n
Funcionamiento: interfaz Gradio traduce correctamente usando Gen-AI. (30%)
MLflow: cada interacciÃ³n queda registrada (params, mÃ©tricas, artifacts). (30%)
Docker: imÃ¡genes construidas, subidas a Docker Hub y ejecutables remotamente. (25%)
DocumentaciÃ³n: entregables claros (screenshots, repo, pasos). (15%)
Restricciones y buenas prÃ¡cticas (pedir cumplir)
No incluir claves dentro de imÃ¡genes ni en repositorios pÃºblicos. Pasar la API key como variable de entorno al ejecutar el contenedor.
Registrar latencia y tamaÃ±o de respuesta en MLflow para poder comparar runs.
Mantener el proyecto reproducible: indicar comandos exactos para build/run/push/pull en el documento entregable.


Responder

Videos
Build a Simple Language Translation App using Python for ... trailer
Build a Simple Language Translation App using Python for ...
Pushing and Pulling to and from Docker Hub trailer
Pushing and Pulling to and from Docker Hub

Hola! Te ayudarÃ© a entender cÃ³mo hacer este taller de traducciÃ³n con Docker. Es totalmente factible hacerlo con un script Python normal (no necesitas notebook necesariamente, aunque puedes usar uno para desarrollo inicial).â€‹

Resumen del Proyecto
Vas a construir una aplicaciÃ³n de traducciÃ³n que integra tres tecnologÃ­as principales: Gradio para la interfaz web, un modelo generativo (OpenAI/Gemini) para las traducciones, y MLflow para registrar cada interacciÃ³n. Todo debe empaquetarse en contenedores Docker individuales y publicarse en Docker Hub.â€‹â€‹

Estructura Recomendada
Parte A: Desarrollo de la App de TraducciÃ³n
Primero desarrolla localmente sin Docker. Crea un archivo app.py que contenga:â€‹

Interfaz Gradio: Un campo de texto para entrada, un selector de idioma objetivo (dropdown), botÃ³n de traducciÃ³n y Ã¡rea de resultadosâ€‹

IntegraciÃ³n con API: Llamadas al SDK de OpenAI o Gemini para obtener traduccionesâ€‹

FunciÃ³n de traducciÃ³n: Que capture texto origen, idioma objetivo y retorne la traducciÃ³n

La estructura bÃ¡sica serÃ­a usar gradio.Interface() con inputs (textbox y dropdown) y outputs (textbox para mostrar la traducciÃ³n).â€‹

Parte B: IntegraciÃ³n con MLflow Tracking
Necesitas levantar un servidor MLflow en un contenedor separado. Para cada traducciÃ³n, debes:â€‹

Crear un run con mlflow.start_run()â€‹

Registrar parÃ¡metros: mlflow.log_param() para idioma_objetivo, modelo usado, texto originalâ€‹

Registrar mÃ©tricas: mlflow.log_metric() para latencia (tiempo de respuesta en ms), longitud de la respuesta traducidaâ€‹

Guardar artifacts opcionales: archivos .txt con pares original/traducciÃ³nâ€‹

El servidor MLflow debe configurarse para apuntar a una URI especÃ­fica (ej: http://mlflow-server:5000).â€‹

Parte C: DockerizaciÃ³n Sin docker-compose
Debes crear dos contenedores que se comuniquen:â€‹

Contenedor 1 - Servidor MLflow:â€‹

Puedes usar imagen oficial o crear Dockerfile personalizado

Exponer puerto 5000

Montar volumen para persistencia de datos

Contenedor 2 - App de Gradio:

Crear Dockerfile con base Python

Instalar dependencias (gradio, mlflow, openai/google-generativeai)

Exponer puerto 7860 (default de Gradio)â€‹

Configurar la app para conectarse al contenedor MLflow

Para comunicaciÃ³n entre contenedores sin docker-compose, usa una red Docker personalizada:â€‹

bash
docker network create traductor-network
Luego ejecuta ambos contenedores en esa red, y la app podrÃ¡ referenciar al servidor MLflow por nombre de contenedor.â€‹

Parte D: PublicaciÃ³n en Docker Hub
DespuÃ©s de construir la imagen local:â€‹â€‹

Build: docker build -t traductor-app:1.0.0 .

Tag: docker tag traductor-app:1.0.0 tu_usuario/traductor-app:1.0.0â€‹

Login: docker login

Push: docker push tu_usuario/traductor-app:1.0.0â€‹â€‹

Para ejecutar remotamente, pasa la API key como variable de entorno:â€‹

bash
docker run -e OPENAI_API_KEY=tu_clave -p 7860:7860 tu_usuario/traductor-app:1.0.0
Comandos Docker Principales
Crear red: docker network create traductor-networkâ€‹

Ejecutar MLflow: docker run -d --name mlflow-server --network traductor-network -p 5000:5000 -v mlflow_data:/mlflow mlflow-serverâ€‹

Ejecutar app: docker run -d --name traductor-app --network traductor-network -p 7860:7860 -e OPENAI_API_KEY=tu_clave traductor-appâ€‹

Entregables
Screenshots: Captura de la UI de Gradio mostrando una traducciÃ³n + captura de la UI de MLflow mostrando el run registrado con parÃ¡metros y mÃ©tricasâ€‹

Repositorio Docker Hub: El nombre completo (ej: tu_usuario/traductor-genai:1.0.0)â€‹

Documento: MÃ¡ximo 1 pÃ¡gina explicando arquitectura, puertos (7860 para Gradio, 5000 para MLflow), cÃ³mo pasar API key, comandos usados, y observaciones sobre latencia/calidadâ€‹

Recomendaciones
No uses notebook para la app final, usa un script Python regular (app.py) que sea mÃ¡s fÃ¡cil de dockerizarâ€‹

Registra timestamp con datetime.now() en cada run de MLflowâ€‹

Calcula latencia midiendo tiempo antes y despuÃ©s de la llamada al modeloâ€‹

Nunca incluyas la API key en el Dockerfile o cÃ³digo; siempre pÃ¡sala como variable de entornoâ€‹

Prueba localmente primero antes de dockerizarâ€‹

La arquitectura final tendrÃ­a dos contenedores independientes en la misma red Docker, donde la app Gradio se conecta al servidor MLflow por nombre de host, todo sin usar docker-compose como especifica el requisito.â€‹

ayudame primero a hacer la parte A y luego seguimos con las otras

Perfecto! Te ayudarÃ© a crear la Parte A paso a paso. Vamos a construir una aplicaciÃ³n de traducciÃ³n funcional con Gradio y OpenAI (o Gemini).â€‹

ConfiguraciÃ³n Inicial
Primero, crea una carpeta para tu proyecto y un archivo app.py.â€‹

InstalaciÃ³n de Dependencias
Crea un archivo requirements.txt con las siguientes dependencias:â€‹

text
gradio
openai
python-dotenv
Instala las dependencias:

bash
pip install -r requirements.txt
ConfiguraciÃ³n de la API Key
Crea un archivo .env en la raÃ­z del proyecto para almacenar tu clave de API de forma segura:â€‹

text
OPENAI_API_KEY=tu_clave_aqui
CÃ³digo de la AplicaciÃ³n
AquÃ­ estÃ¡ el cÃ³digo completo para app.py:â€‹

python
import gradio as gr
import os
from openai import OpenAI
from dotenv import load_dotenv
import time

# Cargar variables de entorno
load_dotenv()

# Inicializar cliente OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Lista de idiomas disponibles
IDIOMAS = [
    "EspaÃ±ol",
    "InglÃ©s",
    "FrancÃ©s",
    "AlemÃ¡n",
    "Italiano",
    "PortuguÃ©s",
    "Chino",
    "JaponÃ©s",
    "Coreano",
    "Ãrabe",
    "Ruso"
]

def traducir_texto(texto_original, idioma_objetivo):
    """
    FunciÃ³n que traduce el texto usando OpenAI API
    
    Args:
        texto_original: Texto a traducir
        idioma_objetivo: Idioma al que se desea traducir
        
    Returns:
        Texto traducido
    """
    if not texto_original or not texto_original.strip():
        return "Por favor, ingresa un texto para traducir."
    
    if not idioma_objetivo:
        return "Por favor, selecciona un idioma objetivo."
    
    try:
        # Crear el prompt para la traducciÃ³n
        prompt = f"Traduce el siguiente texto al {idioma_objetivo}. Solo devuelve la traducciÃ³n, sin explicaciones adicionales:\n\n{texto_original}"
        
        # Llamada a la API de OpenAI
        inicio = time.time()
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",  # Puedes cambiar a gpt-4 si tienes acceso
            messages=[
                {"role": "system", "content": f"Eres un traductor profesional especializado en traducir textos al {idioma_objetivo}."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # Baja temperatura para traducciones mÃ¡s precisas
            max_tokens=1000
        )
        
        fin = time.time()
        latencia = (fin - inicio) * 1000  # Convertir a milisegundos
        
        # Extraer la traducciÃ³n
        traduccion = response.choices[0].message.content.strip()
        
        # Opcional: imprimir info en consola para debug
        print(f"TraducciÃ³n completada en {latencia:.2f}ms")
        print(f"Texto original ({len(texto_original)} caracteres): {texto_original[:50]}...")
        print(f"TraducciÃ³n ({len(traduccion)} caracteres): {traduccion[:50]}...")
        
        return traduccion
        
    except Exception as e:
        return f"Error en la traducciÃ³n: {str(e)}"

# Crear la interfaz de Gradio
with gr.Blocks(title="Traductor con IA") as demo:
    gr.Markdown("# ðŸŒ AplicaciÃ³n de TraducciÃ³n con IA")
    gr.Markdown("Traduce texto a diferentes idiomas usando modelos de lenguaje generativos.")
    
    with gr.Row():
        with gr.Column():
            texto_input = gr.Textbox(
                label="Texto Original",
                placeholder="Escribe o pega el texto que deseas traducir...",
                lines=6,
                max_lines=10
            )
            
            idioma_dropdown = gr.Dropdown(
                choices=IDIOMAS,
                label="Idioma Objetivo",
                value="InglÃ©s",
                interactive=True
            )
            
            boton_traducir = gr.Button("Traducir", variant="primary")
        
        with gr.Column():
            texto_output = gr.Textbox(
                label="Texto Traducido",
                lines=6,
                max_lines=10,
                interactive=False
            )
    
    # Ejemplos predefinidos
    gr.Examples(
        examples=[
            ["Hola, Â¿cÃ³mo estÃ¡s? Me gustarÃ­a aprender mÃ¡s sobre inteligencia artificial.", "InglÃ©s"],
            ["Buenos dÃ­as, necesito ayuda con mi proyecto.", "FrancÃ©s"],
            ["La tecnologÃ­a estÃ¡ cambiando el mundo rÃ¡pidamente.", "AlemÃ¡n"]
        ],
        inputs=[texto_input, idioma_dropdown]
    )
    
    # Conectar el botÃ³n con la funciÃ³n
    boton_traducir.click(
        fn=traducir_texto,
        inputs=[texto_input, idioma_dropdown],
        outputs=texto_output
    )

# Lanzar la aplicaciÃ³n
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",  # Permite acceso desde cualquier IP (importante para Docker)
        server_port=7860,
        share=False  # Cambia a True si quieres un link pÃºblico temporal
    )
Alternativa con Gemini API
Si prefieres usar Google Gemini en lugar de OpenAI, aquÃ­ estÃ¡ la versiÃ³n alternativa:â€‹

python
import gradio as gr
import os
import google.generativeai as genai
from dotenv import load_dotenv
import time

# Cargar variables de entorno
load_dotenv()

# Configurar Gemini API
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-pro')

# Lista de idiomas (igual que antes)
IDIOMAS = [
    "EspaÃ±ol", "InglÃ©s", "FrancÃ©s", "AlemÃ¡n", "Italiano",
    "PortuguÃ©s", "Chino", "JaponÃ©s", "Coreano", "Ãrabe", "Ruso"
]

def traducir_texto(texto_original, idioma_objetivo):
    if not texto_original or not texto_original.strip():
        return "Por favor, ingresa un texto para traducir."
    
    if not idioma_objetivo:
        return "Por favor, selecciona un idioma objetivo."
    
    try:
        prompt = f"Traduce el siguiente texto al {idioma_objetivo}. Solo devuelve la traducciÃ³n:\n\n{texto_original}"
        
        inicio = time.time()
        response = model.generate_content(prompt)
        fin = time.time()
        
        latencia = (fin - inicio) * 1000
        traduccion = response.text.strip()
        
        print(f"TraducciÃ³n completada en {latencia:.2f}ms")
        
        return traduccion
        
    except Exception as e:
        return f"Error en la traducciÃ³n: {str(e)}"

# El resto de la interfaz Gradio es igual...
Estructura del Proyecto
Tu carpeta deberÃ­a verse asÃ­:â€‹

text
traductor-app/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env
Probar la AplicaciÃ³n Localmente
Ejecuta la aplicaciÃ³n:â€‹

bash
python app.py
DeberÃ­as ver algo como:â€‹

text
Running on local URL:  http://127.0.0.1:7860
Abre tu navegador en esa direcciÃ³n y prueba la aplicaciÃ³n:â€‹

Ingresa un texto en el campo "Texto Original"â€‹

Selecciona un idioma del dropdownâ€‹

Haz clic en "Traducir"

VerÃ¡s la traducciÃ³n en el Ã¡rea de resultadosâ€‹

CaracterÃ­sticas Implementadas
Interfaz Gradio con textbox para entrada y salidaâ€‹

Dropdown para selecciÃ³n de idioma objetivoâ€‹

BotÃ³n de traducciÃ³n con estilo primarioâ€‹

Ejemplos predefinidos para probar rÃ¡pidamenteâ€‹

Prompt optimizado para traducciones precisasâ€‹

MediciÃ³n de latencia (Ãºtil para MLflow en Parte B)â€‹

Manejo de errores bÃ¡sicoâ€‹

Preparado para Docker con server_name="0.0.0.0"â€‹

Siguientes Pasos
Una vez que verifiques que la aplicaciÃ³n funciona correctamente localmente, estarÃ¡s listo para la Parte B donde integraremos MLflow Tracking para registrar cada traducciÃ³n. AvÃ­same cuando quieras continuar con esa parte!â€‹

si utilizo el de gemini, deberia instalar alguna otra libreria?

SÃ­, necesitas instalar librerÃ­as adicionales para usar Gemini. Hay dos opciones dependiendo de quÃ© SDK quieras usar:â€‹

OpciÃ³n 1: SDK Nuevo (Recomendado para 2025)
Google ahora tiene un SDK unificado mÃ¡s moderno llamado google-genai:â€‹

bash
pip install google-genai
Tu requirements.txt quedarÃ­a asÃ­:â€‹

text
gradio
google-genai
python-dotenv
Y el cÃ³digo serÃ­a:â€‹

python
from google import genai
from dotenv import load_dotenv
import os

load_dotenv()

# Crear cliente con la API key
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))

def traducir_texto(texto_original, idioma_objetivo):
    # ... validaciones ...
    
    try:
        prompt = f"Traduce el siguiente texto al {idioma_objetivo}. Solo devuelve la traducciÃ³n:\n\n{texto_original}"
        
        response = client.models.generate_content(
            model='gemini-1.5-flash',
            contents=prompt
        )
        
        traduccion = response.text.strip()
        return traduccion
        
    except Exception as e:
        return f"Error: {str(e)}"
OpciÃ³n 2: SDK Anterior (MÃ¡s DocumentaciÃ³n Disponible)
El SDK anterior google-generativeai tambiÃ©n sigue funcionando:â€‹

bash
pip install google-generativeai
Tu requirements.txt quedarÃ­a asÃ­:â€‹

text
gradio
google-generativeai
python-dotenv
Y el cÃ³digo serÃ­a:â€‹

python
import google.generativeai as genai
from dotenv import load_dotenv
import os

load_dotenv()

# Configurar la API
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel('gemini-pro')

def traducir_texto(texto_original, idioma_objetivo):
    # ... validaciones ...
    
    try:
        prompt = f"Traduce el siguiente texto al {idioma_objetivo}. Solo devuelve la traducciÃ³n:\n\n{texto_original}"
        
        response = model.generate_content(prompt)
        traduccion = response.text.strip()
        return traduccion
        
    except Exception as e:
        return f"Error: {str(e)}"
ConfiguraciÃ³n del API Key
En tu archivo .env, necesitas poner:â€‹

text
GOOGLE_API_KEY=tu_clave_de_gemini_aqui
Importante: El nombre de la variable de entorno puede ser GOOGLE_API_KEY o GEMINI_API_KEY (ambos funcionan, pero GOOGLE_API_KEY tiene precedencia).â€‹

Requisitos de Python
AsegÃºrate de tener Python 3.9 o superior, ya que estos SDKs requieren versiones recientes.â€‹

Resumen
Para Gemini necesitas instalar:â€‹

Nuevo SDK: google-genai (recomendado)

SDK anterior: google-generativeai (mÃ¡s ejemplos disponibles)

ComÃºn a ambos: python-dotenv (para manejar variables de entorno)â€‹

ComÃºn a ambos: gradio (para la interfaz)

Te recomendarÃ­a usar google-generativeai (opciÃ³n 2) porque tiene mÃ¡s ejemplos y documentaciÃ³n disponible actualmente, aunque el nuevo SDK google-genai es el oficial de Google para el futuro.â€‹

ya hice todo eso, como hago para ejectuar en "remoto" pero desde mi pc para probar el repo en docker hub

Perfecto, vamos a simular una ejecuciÃ³n "remota" desde tu misma PC para verificar que funciona desde Docker Hub.

Paso 1: Publicar en Docker Hub
powershell
# 1. Login en Docker Hub
docker login

# Te pedirÃ¡ usuario y contraseÃ±a de Docker Hub
# Si no tienes cuenta, crÃ©ala en https://hub.docker.com
Paso 2: Taggear la Imagen
powershell
# Reemplaza "tu_usuario" con tu usuario real de Docker Hub
docker tag traductor-app:1.0.0 tu_usuario/traductor-genai:1.0.0

# Verificar que se taggeÃ³
docker images | findstr traductor
DeberÃ­as ver:

text
tu_usuario/traductor-genai    1.0.0    abc123def456    2 hours ago    1.2GB
traductor-app                 1.0.0    abc123def456    2 hours ago    1.2GB
Paso 3: Push a Docker Hub
powershell
# Subir la imagen
docker push tu_usuario/traductor-genai:1.0.0

# DeberÃ­a mostrar:
# The push refers to repository [docker.io/tu_usuario/traductor-genai]
# ...
# 1.0.0: digest: sha256:xxxxx size: xxxxx
Verifica en https://hub.docker.com/r/tu_usuario/traductor-genai

Paso 4: Simular EjecuciÃ³n "Remota"
Ahora simula que es otra mÃ¡quina limpia:

powershell
# PASO A: Detener y eliminar TODO lo local
docker stop mlflow-server traductor-app
docker rm mlflow-server traductor-app
docker rmi mlflow-server:latest traductor-app:1.0.0

# PASO B: Limpiar volumen (empezar desde cero)
docker volume rm mlflow_data

# PASO C: Borrar la red
docker network rm traductor-network

# Verificar que no queda nada
docker ps -a
docker images | findstr traductor
Paso 5: Ejecutar Como si Fuera "Remoto"
Ahora ejecuta exactamente como lo harÃ­a alguien en otra mÃ¡quina:

powershell
# 1. Crear red
docker network create traductor-network

# 2. Ejecutar MLflow (desde imagen local, porque no estÃ¡ en Docker Hub)
docker build -f Dockerfile.mlflow -t mlflow-server:latest .

docker run -d `
  --name mlflow-server `
  --network traductor-network `
  -p 5000:5000 `
  -v mlflow_data:/mlflow/mlruns `
  -e MLFLOW_DISABLE_HOST_HEADER_VALIDATION=true `
  mlflow-server:latest

# 3. Esperar a que MLflow estÃ© listo
Start-Sleep -Seconds 10

# 4. AQUÃ VIENE LO "REMOTO": Pull y ejecutar desde Docker Hub
docker pull tu_usuario/traductor-genai:1.0.0

# 5. Ejecutar la app descargada desde Docker Hub
docker run -d `
  --name traductor-app `
  --network traductor-network `
  -p 7860:7860 `
  -e "GOOGLE_API_KEY=tu_clave_real_aqui" `
  -e "MLFLOW_TRACKING_URI=http://mlflow-server:5000" `
  tu_usuario/traductor-genai:1.0.0

# 6. Esperar a que inicie
Start-Sleep -Seconds 5

# 7. Verificar que estÃ¡ corriendo
docker ps
docker logs traductor-app
Paso 6: Probar Funcionalidad
powershell
# Abrir interfaces
Start-Process http://localhost:7860  # Gradio
Start-Process http://localhost:5000  # MLflow

# Hacer una traducciÃ³n en Gradio
# Verificar que aparece en MLflow
Paso 7: Verificar Logs
powershell
# Ver logs de la app
docker logs -f traductor-app

# Si ves errores, copiar el error completo
Paso 8: Documentar lo que PasÃ³
Toma screenshots de:

Comando de pull:

powershell
docker pull tu_usuario/traductor-genai:1.0.0
Comando de run:

powershell
docker run -d ... tu_usuario/traductor-genai:1.0.0
Gradio funcionando (con una traducciÃ³n)

MLflow mostrando el run (con parÃ¡metros/mÃ©tricas)

Alternativa: Prueba en WSL o Virtual Machine
Si quieres ser mÃ¡s realista, simula en WSL2 (Windows Subsystem for Linux):

powershell
# En PowerShell:
wsl --list

# Entrar a WSL
wsl

# Dentro de WSL (bash):
docker pull tu_usuario/traductor-genai:1.0.0
docker network create traductor-network
docker run -d --name mlflow-server -p 5000:5000 -v mlflow_data:/mlflow/mlruns mlflow-server:latest
docker run -d --name traductor-app -p 7860:7860 -e GOOGLE_API_KEY=xxx -e MLFLOW_TRACKING_URI=http://mlflow-server:5000 tu_usuario/traductor-genai:1.0.0
Luego accede desde Windows a http://localhost:7860

Ejemplo Completo (Copia y Pega)
powershell
# ========== SIMULACIÃ“N "REMOTA" ==========

# Limpiar todo
docker stop mlflow-server traductor-app 2>$null
docker rm mlflow-server traductor-app 2>$null
docker rmi mlflow-server:latest traductor-app:1.0.0 2>$null
docker volume rm mlflow_data 2>$null
docker network rm traductor-network 2>$null

# Crear red
docker network create traductor-network

# MLflow
docker build -f Dockerfile.mlflow -t mlflow-server:latest .
docker run -d --name mlflow-server --network traductor-network -p 5000:5000 -v mlflow_data:/mlflow/mlruns -e MLFLOW_DISABLE_HOST_HEADER_VALIDATION=true mlflow-server:latest

Start-Sleep -Seconds 10

# App desde Docker Hub (LO "REMOTO")
docker pull tu_usuario/traductor-genai:1.0.0
docker run -d --name traductor-app --network traductor-network -p 7860:7860 -e "GOOGLE_API_KEY=tu_clave_aqui" -e "MLFLOW_TRACKING_URI=http://mlflow-server:5000" tu_usuario/traductor-genai:1.0.0

Start-Sleep -Seconds 5

# Verificar
docker ps
docker logs traductor-app

# Abrir
Start-Process http://localhost:7860
Start-Process http://localhost:5000
Â¿Ejecutas esto y me dices si funciona?

esta demorando subir creo talvez eso no deberia subirlo
(.venv) PS C:\Users\Christian\Documents\mlflowdocker> docker push ford17/traductor-genai:1.0.0                  
The push refers to repository [docker.io/ford17/traductor-genai]
95a981011ac8: Pushed
316e93cde934: Pushing  69.19MB/930.3MB
bf62238f6664: Pushed
bde6ba8e1c3a: Pushed
72ad42e849c7: Mounted from library/python
29fdaa774d32: Mounted from library/python
4f848346ede3: Mounted from library/python
d7c97cb6f1fe: Mounted from library/python

SÃ­, estÃ¡ subiendo las capas de Python. Eso es normal pero lento. La imagen es grande (~1.2GB) porque incluye Python y todas las dependencias.

Opciones:
OpciÃ³n 1: Esperar (MÃ¡s Simple)
DÃ©jalo subir, puede tardar 5-15 minutos dependiendo de tu internet. Mientras esperas, puedes:

powershell
# En otra terminal, ver el progreso
docker push ford17/traductor-genai:1.0.0

# Ver imÃ¡genes en tu cuenta Docker Hub
# https://hub.docker.com/r/ford17/traductor-genai
OpciÃ³n 2: Optimizar la Imagen (Recomendado para Futuro)
Si quieres una imagen mÃ¡s pequeÃ±a, modifica el Dockerfile:

text
FROM python:3.10-slim

WORKDIR /app

# Instalar solo lo necesario
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt && \
    rm -rf /root/.cache/pip

COPY app.py .

EXPOSE 7860

ENV PYTHONUNBUFFERED=1

CMD ["python", "app.py"]
La diferencia:

python:3.10 â†’ ~900MB

python:3.10-slim â†’ ~150MB (ya lo usas)

O usa python:3.10-alpine â†’ ~50MB (pero mÃ¡s complicado)

OpciÃ³n 3: Multi-stage Build (MÃ¡s Avanzado)
text
# Stage 1: Build
FROM python:3.10-slim as builder

WORKDIR /app
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# Stage 2: Runtime
FROM python:3.10-slim

WORKDIR /app

# Copiar solo lo necesario del builder
COPY --from=builder /root/.local /root/.local
COPY app.py .

ENV PATH=/root/.local/bin:$PATH
ENV PYTHONUNBUFFERED=1

EXPOSE 7860

CMD ["python", "app.py"]
Esto reduce la imagen a ~400-500MB.

Por Ahora:
Deja que suba (Ctrl+C si quieres cancelar). Una vez termine:

powershell
# Ver en tu Docker Hub
Start-Process https://hub.docker.com/r/ford17/traductor-genai

# Una vez estÃ© (en 10-15 min), ejecuta la prueba "remota"
Mientras tanto, prepara el documento entregable y las capturas de pantalla.

Â¿Quieres que mientras espera el push, preparemos el README final con los comandos exactos?

vale ya tengo las capturas de pantalla dame el readme con los comandos exactos y dame mis comandos para subirlo a mi github C-Ford17/Laboratorio-MLFLOW-with-Docker

Perfecto, aquÃ­ estÃ¡ el README.md completo y los comandos para GitHub:

1. Crea README.md en tu proyecto
text
# Laboratorio: App de TraducciÃ³n con Docker y MLflow

AplicaciÃ³n de traducciÃ³n de texto usando **Gemini API** con tracking en **MLflow**, empaquetada en **contenedores Docker** y publicada en **Docker Hub**.

## ðŸ“‹ DescripciÃ³n

Esta aplicaciÃ³n permite traducir texto a mÃºltiples idiomas utilizando el modelo Gemini 2.5-flash. Cada traducciÃ³n se registra automÃ¡ticamente en MLflow para tracking, incluyendo parÃ¡metros, mÃ©tricas y artifacts.

**CaracterÃ­sticas:**
- âœ… Interfaz web con Gradio
- âœ… Traducciones con Gemini API
- âœ… Tracking completo en MLflow
- âœ… DockerizaciÃ³n sin docker-compose
- âœ… PublicaciÃ³n en Docker Hub
- âœ… EjecuciÃ³n remota

## ðŸ—ï¸ Arquitectura

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docker Network â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ traductor-app â”‚ â”‚ mlflow-server â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ Puerto: 7860 â”‚â”€â”€â”€â”€â†’â”‚ Puerto: 5000 â”‚ â”‚
â”‚ â”‚ Gradio UI â”‚ â”‚ MLflow Tracking UI â”‚ â”‚
â”‚ â”‚ (Python 3.10) â”‚ â”‚ (Python 3.10) â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â†“ â”‚
â”‚ Volumen: mlflow_data â”‚
â”‚ (/mlflow/mlruns) â”‚
â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

text

## ðŸš€ Inicio RÃ¡pido

### Requisitos
- Docker 20.10+
- Docker Hub account (para publicaciÃ³n)
- Google Gemini API key ([obtener aquÃ­](https://ai.google.dev/))

### Paso 1: Clonar el repositorio

git clone https://github.com/C-Ford17/Laboratorio-MLFLOW-with-Docker.git
cd Laboratorio-MLFLOW-with-Docker

text

### Paso 2: Crear archivo `.env`

echo "GOOGLE_API_KEY=tu_clave_gemini_aqui" > .env


**âš ï¸ IMPORTANTE:** Nunca commitear `.env` a Git. Ya estÃ¡ en `.gitignore`.

### Paso 3: Ejecutar con Docker (Sin docker-compose)

#### A. Crear red
docker network create traductor-network

#### B. Levantar MLflow
docker build -f Dockerfile.mlflow -t mlflow-server:latest .

docker run -d
--name mlflow-server
--network traductor-network
-p 5000:5000
-v mlflow_data:/mlflow/mlruns
-e MLFLOW_DISABLE_HOST_HEADER_VALIDATION=true
mlflow-server:latest

#### C. Construir imagen de la app
docker build -t traductor-app:1.0.0 .

#### D. Ejecutar la app
docker run -d
--name traductor-app
--network traductor-network
-p 7860:7860
-e GOOGLE_API_KEY=$(cat .env | grep GOOGLE_API_KEY | cut -d '=' -f 2)
-e MLFLOW_TRACKING_URI=http://mlflow-server:5000
traductor-app:1.0.0

#### E. Acceder
- **Gradio UI:** http://localhost:7860
- **MLflow Dashboard:** http://localhost:5000

## ðŸ“¦ PublicaciÃ³n en Docker Hub

### Paso 1: Login
docker login

Ingresar usuario y contraseÃ±a

### Paso 2: Taggear imagen
docker tag traductor-app:1.0.0 ford17/traductor-genai:1.0.0
docker tag traductor-app:1.0.0 ford17/traductor-genai:latest

### Paso 3: Push

docker push ford17/traductor-genai:1.0.0
docker push ford17/traductor-genai:latest

Verificar en: https://hub.docker.com/r/ford17/traductor-genai

## ðŸŒ EjecuciÃ³n Remota (Desde Docker Hub)

### OpciÃ³n 1: Misma mÃ¡quina (simular remoto)

Limpiar todo
docker stop mlflow-server traductor-app 2>/dev/null
docker rm mlflow-server traductor-app 2>/dev/null
docker rmi mlflow-server:latest traductor-app:1.0.0 2>/dev/null
docker volume rm mlflow_data 2>/dev/null
docker network rm traductor-network 2>/dev/null

Crear red
docker network create traductor-network

MLflow
docker build -f Dockerfile.mlflow -t mlflow-server:latest .
docker run -d
--name mlflow-server
--network traductor-network
-p 5000:5000
-v mlflow_data:/mlflow/mlruns
-e MLFLOW_DISABLE_HOST_HEADER_VALIDATION=true
mlflow-server:latest

sleep 10

### App desde Docker Hub
docker pull ford17/traductor-genai:1.0.0

docker run -d
--name traductor-app
--network traductor-network
-p 7860:7860
-e GOOGLE_API_KEY=tu_clave_aqui
-e MLFLOW_TRACKING_URI=http://mlflow-server:5000
ford17/traductor-genai:1.0.0

sleep 5

Verificar
docker ps
docker logs traductor-app

### OpciÃ³n 2: Otra mÃ¡quina
En mÃ¡quina remota
docker pull ford17/traductor-genai:1.0.0
docker network create traductor-network

Levantar MLflow en mÃ¡quina remota (o usar remoto)
docker run -d --name mlflow-server --network traductor-network -p 5000:5000 -v mlflow_data:/mlflow/mlruns mlflow-server:latest

Ejecutar app
docker run -d
--name traductor-app
--network traductor-network
-p 7860:7860
-e GOOGLE_API_KEY=tu_clave
-e MLFLOW_TRACKING_URI=http://mlflow-server:5000
ford17/traductor-genai:1.0.0


## ðŸ“Š Datos Registrados en MLflow

Cada traducciÃ³n registra:

**ParÃ¡metros:**
- `idioma_objetivo`: Idioma seleccionado
- `modelo`: gemini-2.5-flash
- `temperatura`: 0.1
- `prompt_hash`: Hash del texto original
- `len_texto_original`: Longitud del texto

**MÃ©tricas:**
- `latency_ms`: Tiempo de respuesta (ms)
- `len_response`: Caracteres de la traducciÃ³n
- `tokens_aprox`: Tokens aproximados

**Artifacts:**
- `traduccion.txt`: Pareja original/traducciÃ³n

## ðŸ› ï¸ Comandos Ãštiles

Ver contenedores corriendo
docker ps

Ver logs
docker logs -f traductor-app
docker logs -f mlflow-server

Detener contenedores
docker stop traductor-app mlflow-server

Eliminar contenedores
docker rm traductor-app mlflow-server

Eliminar volumen
docker volume rm mlflow_data

Eliminar red
docker network rm traductor-network

Limpiar todo
docker system prune -a


## ðŸ“ˆ Observaciones de Rendimiento

- **Latencia promedio:** 1.3-1.5 segundos
- **Calidad de traducciÃ³n:** Excelente (Gemini 2.5-flash)
- **TamaÃ±o imagen:** ~1.2GB (python:3.10-slim + deps)
- **Memoria RAM:** ~500MB por contenedor
- **Almacenamiento MLflow:** ~10KB por run

## ðŸ“ Estructura del Proyecto

Laboratorio-MLFLOW-with-Docker/
â”œâ”€â”€ app.py # CÃ³digo principal (Gradio + MLflow)
â”œâ”€â”€ Dockerfile # Imagen app
â”œâ”€â”€ Dockerfile.mlflow # Imagen MLflow
â”œâ”€â”€ docker-compose.yml # OrquestaciÃ³n (referencia)
â”œâ”€â”€ requirements.txt # Dependencias Python
â”œâ”€â”€ .env # Variables de entorno (NO commitear)
â”œâ”€â”€ .gitignore # Archivos a ignorar
â”œâ”€â”€ README.md # Este archivo
â”œâ”€â”€ DOCUMENTACION.md # DocumentaciÃ³n tÃ©cnica
â””â”€â”€ mlruns/ # Datos MLflow (generado)


## ðŸ” Seguridad

- âœ… API key pasada como variable de entorno
- âœ… `.env` en `.gitignore`
- âœ… No hay credenciales en imÃ¡genes
- âœ… VolÃºmenes Docker con datos persistentes

## ðŸ“ Problemas Comunes

### "Connection refused" en MLflow
â†’ Esperar 10+ segundos a que MLflow inicie

### "Invalid Host header" en MLflow
â†’ Variable `MLFLOW_DISABLE_HOST_HEADER_VALIDATION=true` no configurada

### Archivo `.txt` no aparece en MLflow
â†’ Verificar que el volumen estÃ¡ compartido correctamente

### Docker Hub push lento
â†’ Normal para imÃ¡genes >1GB. Paciencia o reducir tamaÃ±o con multi-stage build

## ðŸ“š Referencias

- [Gradio Docs](https://www.gradio.app/)
- [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html)
- [Docker Compose](https://docs.docker.com/compose/)
- [Google Gemini API](https://ai.google.dev/)
- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)

## ðŸ‘¤ Autor

**Christian Ford** - [GitHub](https://github.com/C-Ford17)

## ðŸ“„ Licencia

Este proyecto es de cÃ³digo abierto bajo licencia MIT.

---

**Ãšltima actualizaciÃ³n:** Noviembre 2, 2025
